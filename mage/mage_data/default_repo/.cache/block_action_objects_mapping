{"block_file": {"custom/testvar.py:custom:python:testvar": {"content": "import os\nfor k, v in sorted(os.environ.items()):\n    if k.startswith('SNOWFLAKE_'):\n        mask = v if ('PASSWORD' not in k) else '***'\n        print(f'{k} = {mask}')\n", "file_path": "custom/testvar.py", "language": "python", "type": "custom", "uuid": "testvar"}, "data_exporters/copy_into_bronze.py:data_exporter:python:copy into bronze": {"content": "# --- guard del template de Mage ---\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nfrom pandas import DataFrame\nimport pandas as pd\nimport uuid\nimport time, requests, tempfile, os, logging, math\n\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nimport pyarrow.parquet as pq\nimport pyarrow as pa\n\n# Silenciar logs ruidosos de Snowflake\nlogging.getLogger('snowflake.connector').setLevel(logging.WARNING)\nlogging.getLogger('snowflake.connector.ocsp_snowflake').setLevel(logging.ERROR)\nlogging.getLogger('snowflake.connector.file_transfer_agent').setLevel(logging.ERROR)\n\n# ===================== DDL BRONZE (ingest_ts como STRING) =====================\nYELLOW_DDL = \"\"\"\ncreate table if not exists {db}.{schema}.yellow_trips (\n    vendorid integer,\n    tpep_pickup_datetime timestamp,\n    tpep_dropoff_datetime timestamp,\n    passenger_count integer,\n    trip_distance float,\n    ratecodeid integer,\n    store_and_fwd_flag string,\n    pulocationid integer,\n    dolocationid integer,\n    payment_type integer,\n    fare_amount float,\n    extra float,\n    mta_tax float,\n    tip_amount float,\n    tolls_amount float,\n    improvement_surcharge float,\n    total_amount float,\n    congestion_surcharge float,\n    airport_fee float,\n    cbd_congestion_fee float,\n    -- metadatos\n    run_id string,\n    ingest_ts string,    -- ISO string\n    year int,\n    month int,\n    service_type string,\n    source_url string\n);\n\"\"\"\n\nGREEN_DDL = \"\"\"\ncreate table if not exists {db}.{schema}.green_trips (\n    vendorid integer,\n    lpep_pickup_datetime timestamp,\n    lpep_dropoff_datetime timestamp,\n    passenger_count integer,\n    trip_distance float,\n    ratecodeid integer,\n    store_and_fwd_flag string,\n    pulocationid integer,\n    dolocationid integer,\n    payment_type integer,\n    fare_amount float,\n    extra float,\n    mta_tax float,\n    tip_amount float,\n    tolls_amount float,\n    improvement_surcharge float,\n    total_amount float,\n    congestion_surcharge float,\n    trip_type integer,\n    cbd_congestion_fee float,\n    ehail_fee float,\n    -- metadatos\n    run_id string,\n    ingest_ts string,    -- ISO string\n    year int,\n    month int,\n    service_type string,\n    source_url string\n);\n\"\"\"\n\n# ===================== Columnas esperadas =====================\nYELLOW_COLS = [\n    'vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance',\n    'ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount',\n    'extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount',\n    'congestion_surcharge','airport_fee','cbd_congestion_fee'\n]\nGREEN_COLS = [\n    'vendorid','lpep_pickup_datetime','lpep_dropoff_datetime','passenger_count','trip_distance',\n    'ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount',\n    'extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount',\n    'congestion_surcharge','trip_type','cbd_congestion_fee','ehail_fee'\n]\nMETA_COLS = ['run_id','ingest_ts','year','month','service_type','source_url']\n\n# ===================== Conexi\u00f3n Snowflake =====================\ndef _conn():\n    return snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_RAW'),\n        client_session_keep_alive=False,\n        ocsp_fail_open=True,\n        insecure_mode=True,\n    )\n\n# ===================== Utilidades =====================\ndef _download_parquet(url: str, timeout_connect=8, timeout_read=90) -> str:\n    headers = {'User-Agent': 'mage-ai/nyc-tlc-pipeline'}\n    with requests.get(url, headers=headers, stream=True, timeout=(timeout_connect, timeout_read)) as r:\n        r.raise_for_status()\n        fd, tmp_path = tempfile.mkstemp(suffix='.parquet'); os.close(fd)\n        with open(tmp_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                if chunk: f.write(chunk)\n    return tmp_path\n\ndef _normalize_trip_datetimes(pdf: pd.DataFrame, service: str) -> None:\n    \"\"\"\n    Convierte pickup/dropoff a 'YYYY-MM-DD HH:MM:SS' como string (Snowflake TIMESTAMP_NTZ friendly).\n    \"\"\"\n    if service == 'yellow':\n        dt_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n    else:\n        dt_cols = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n    for c in dt_cols:\n        dt = pd.to_datetime(pdf[c], errors='coerce', utc=False)\n        iso = dt.dt.strftime('%Y-%m-%d %H:%M:%S')\n        pdf[c] = iso\n        pdf.loc[dt.isna(), c] = None\n\n# ===================== Exportador principal =====================\n@data_exporter\ndef export_data(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Input (desde bloque 2): ['year','month','service_type','url','has_parquet', ...]\n    - Crea tablas con ingest_ts como STRING (ISO)\n    - Idempotencia por (service, year, month): DELETE previo\n    - Descarga parquet, lee por row group, sube en micro-batches\n    - Normaliza columnas y fechas (pickup/dropoff ISO; ingest_ts ISO)\n    kwargs:\n      - batch_size_yellow (int, default 100_000)\n      - batch_size_green  (int, default 600_000)\n    \"\"\"\n    if df is None or len(df) == 0:\n        print('No hay filas de entrada.'); return\n    df = df[df['has_parquet'] == True].copy()\n    if df.empty:\n        print('No hay archivos Parquet disponibles para cargar.'); return\n\n    DB = get_secret_value('SNOWFLAKE_DATABASE')\n    SCHEMA_RAW = get_secret_value('SNOWFLAKE_SCHEMA_RAW')\n\n    df['year'] = df['year'].astype(int)\n    df['month'] = df['month'].astype(int)\n    df['service_type'] = df['service_type'].astype(str)\n\n    bs_yellow = int(kwargs.get('batch_size_yellow', 400_000))\n    bs_green  = int(kwargs.get('batch_size_green',  600_000))\n\n    conn = _conn()\n    try:\n        cs = conn.cursor()\n        # Crear tablas si no existen\n        cs.execute(YELLOW_DDL.format(db=DB, schema=SCHEMA_RAW))\n        cs.execute(GREEN_DDL.format(db=DB, schema=SCHEMA_RAW))\n        # Asegurar columnas recientes\n        cs.execute(f\"alter table if exists {DB}.{SCHEMA_RAW}.yellow_trips add column if not exists cbd_congestion_fee float\")\n        cs.execute(f\"alter table if exists {DB}.{SCHEMA_RAW}.green_trips  add column if not exists cbd_congestion_fee float\")\n        cs.execute(f\"alter table if exists {DB}.{SCHEMA_RAW}.green_trips  add column if not exists ehail_fee float\")\n\n        for (service, year, month), part in df.groupby(['service_type', 'year', 'month']):\n            urls = part['url'].tolist()\n            table_name = f'{service}_trips'\n            fq_table = f'{DB}.{SCHEMA_RAW}.{table_name}'\n\n            # Cursor fresco por iteraci\u00f3n\n            try:\n                cs = conn.cursor()\n            except Exception:\n                try: conn.close()\n                except Exception: pass\n                conn = _conn(); cs = conn.cursor()\n\n            # Idempotencia por lote (replace de partici\u00f3n natural)\n            cs.execute(\n                f\"delete from {fq_table} where year = %s and month = %s and service_type = %s\",\n                (year, month, service)\n            )\n\n            run_id = str(uuid.uuid4())\n            total_rows = 0\n\n            for url in urls:\n                try:\n                    print(f\"[{service} {year}-{month:02d}] Descargando: {url}\")\n                    local_path = _download_parquet(url)\n\n                    pf = pq.ParquetFile(local_path)\n                    num_groups = pf.num_row_groups\n                    print(f\"[{service} {year}-{month:02d}] Row groups: {num_groups}\")\n\n                    for rg in range(num_groups):\n                        tbl: pa.Table = pf.read_row_group(rg)\n                        num_rows = tbl.num_rows\n                        batch_size = bs_yellow if service == 'yellow' else bs_green\n                        num_batches = max(1, math.ceil(num_rows / batch_size))\n\n                        for b in range(num_batches):\n                            t0 = time.time()\n                            start = b * batch_size\n                            end = min((b + 1) * batch_size, num_rows)\n                            slice_tbl: pa.Table = tbl.slice(offset=start, length=end - start)\n                            pdf = slice_tbl.to_pandas(split_blocks=True, self_destruct=True)\n\n                            # normalizar columnas\n                            pdf.columns = [str(c).lower() for c in pdf.columns]\n                            base_cols = YELLOW_COLS if service == 'yellow' else GREEN_COLS\n                            for c in base_cols:\n                                if c not in pdf.columns:\n                                    pdf[c] = pd.NA\n\n                            # metadatos (ingest_ts ISO string)\n                            pdf['run_id'] = run_id\n                            pdf['ingest_ts'] = pd.Timestamp.utcnow().tz_localize(None).strftime('%Y-%m-%d %H:%M:%S')\n                            pdf['year'] = year\n                            pdf['month'] = month\n                            pdf['service_type'] = service\n                            pdf['source_url'] = url\n\n                            # normalizar fechas pickup/dropoff a ISO\n                            _normalize_trip_datetimes(pdf, service)\n\n                            # orden final\n                            pdf = pdf[base_cols + META_COLS]\n\n                            ok, nchunks, nrows, _ = write_pandas(\n                                conn, pdf,\n                                table_name=table_name,\n                                database=DB,\n                                schema=SCHEMA_RAW,\n                                quote_identifiers=False,\n                                chunk_size=100_000,\n                            )\n                            total_rows += nrows\n                            print(f\"[{service} {year}-{month:02d}] RG {rg+1}/{num_groups} | batch {b+1}/{num_batches} \u2192 rows={nrows} ({round(time.time()-t0,1)}s)\")\n\n                    os.remove(local_path)\n                except Exception as e:\n                    print(f\"[{service} {year}-{month:02d}] Error: {e}\")\n\n            print(f\"[{service} {year}-{month:02d}] Total subido: {total_rows} filas\")\n\n    finally:\n        try: cs.close()\n        except Exception: pass\n        conn.close()\n", "file_path": "data_exporters/copy_into_bronze.py", "language": "python", "type": "data_exporter", "uuid": "copy_into_bronze"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/load_taxi_zones.py:data_exporter:python:load taxi zones": {"content": "# --- guard del template de Mage ---\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nimport pandas as pd\nimport requests, tempfile, os, logging\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\n\nlogging.getLogger('snowflake.connector').setLevel(logging.WARNING)\n\n# URLs candidatas (la CDN de TLC a veces cambia el nombre)\nCANDIDATE_URLS = [\n    \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\",\n    \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\",  # fallback\n]\n\nDDL_ZONES = \"\"\"\ncreate table if not exists {db}.{schema}.taxi_zones (\n    locationid int,\n    borough string,\n    zone string,\n    service_zone string\n);\n\"\"\"\n\ndef _conn():\n    return snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_RAW'),\n        client_session_keep_alive=False,\n        ocsp_fail_open=True,\n        insecure_mode=True,\n    )\n\ndef _download_csv() -> str:\n    last_err = None\n    for url in CANDIDATE_URLS:\n        try:\n            r = requests.get(url, timeout=(5, 60))\n            r.raise_for_status()\n            fd, tmp = tempfile.mkstemp(suffix='.csv'); os.close(fd)\n            with open(tmp, 'wb') as f:\n                f.write(r.content)\n            print(f\"[taxi_zones] Descargado desde: {url}\")\n            return tmp\n        except Exception as e:\n            last_err = e\n            print(f\"[taxi_zones] Intento fallido {url}: {e}\")\n    raise RuntimeError(f\"No se pudo descargar Taxi Zones: {last_err}\")\n\n@data_exporter\ndef export_data(*args, **kwargs) -> None:\n    DB = get_secret_value('SNOWFLAKE_DATABASE')\n    SCHEMA = get_secret_value('SNOWFLAKE_SCHEMA_RAW')  # BRONZE\n\n    # 1) Descargar CSV\n    csv_path = _download_csv()\n\n    # 2) Leer y normalizar\n    df = pd.read_csv(csv_path)\n    os.remove(csv_path)\n\n    # normalizar nombres\n    df.columns = [c.strip().lower() for c in df.columns]\n    # renombrar si vinieran con may\u00fasculas o espacios\n    rename_map = {\n        'locationid': 'locationid',\n        'borough': 'borough',\n        'zone': 'zone',\n        'service_zone': 'service_zone',\n    }\n    df = df.rename(columns=rename_map)\n\n    # asegurar columnas\n    for c in ['locationid', 'borough', 'zone', 'service_zone']:\n        if c not in df.columns:\n            df[c] = pd.NA\n\n    df = df[['locationid', 'borough', 'zone', 'service_zone']].copy()\n\n    # tipificar\n    df['locationid'] = pd.to_numeric(df['locationid'], errors='coerce').astype('Int64')\n\n    # 3) Crear tabla si no existe\n    conn = _conn()\n    cs = conn.cursor()\n    try:\n        cs.execute(DDL_ZONES.format(db=DB, schema=SCHEMA))\n\n        # 4) Idempotencia: reemplazar contenido\n        cs.execute(f\"truncate table {DB}.{SCHEMA}.taxi_zones\")\n\n        # 5) Cargar\n        ok, nchunks, nrows, _ = write_pandas(\n            conn,\n            df,\n            table_name='taxi_zones',\n            database=DB,\n            schema=SCHEMA,\n            quote_identifiers=False,\n            chunk_size=10_000,\n        )\n        print(f\"[taxi_zones] Cargado ok={ok}, filas={nrows}, chunks={nchunks}\")\n    finally:\n        try: cs.close()\n        except Exception: pass\n        conn.close()\n", "file_path": "data_exporters/load_taxi_zones.py", "language": "python", "type": "data_exporter", "uuid": "load_taxi_zones"}, "data_exporters/sync_coverage_to_audit_py.py:data_exporter:python:sync coverage to audit py": {"content": "# --- guard del template de Mage ---\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom mage_ai.settings.repo import get_repo_path\n\nimport os\nimport pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\n\ndef _conn():\n    return snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_RAW'),  # BRONZE\n        client_session_keep_alive=False,\n        ocsp_fail_open=True,\n        insecure_mode=True,\n    )\n\nDDL_AUDIT = \"\"\"\ncreate table if not exists {db}.{schema}.load_audit (\n  service_type string,\n  year int,\n  month int,\n  row_count number,\n  latest_ingest_ts timestamp,\n  status string,   -- OK | PENDING | MISSING\n  note string\n);\n\"\"\"\n\n@data_exporter\ndef export_data(*args, **kwargs) -> None:\n    \"\"\"\n    Lee coverage_matrix.csv y sincroniza BRONZE.LOAD_AUDIT:\n      - MISSING: has_parquet=false\n      - PENDING: has_parquet=true y row_count=0\n      - OK:      has_parquet=true y row_count>0\n    No reingesta datos; s\u00f3lo consulta conteos en BRONZE.Y/G y reconstruye LOAD_AUDIT.\n    \"\"\"\n    DB = get_secret_value('SNOWFLAKE_DATABASE')\n    SCHEMA = get_secret_value('SNOWFLAKE_SCHEMA_RAW')  # BRONZE\n\n    # 1) Cargar coverage_matrix.csv del repo\n    repo = get_repo_path()\n    coverage_path = os.path.join(repo, 'coverage_matrix.csv')\n    if not os.path.exists(coverage_path):\n        raise FileNotFoundError(f\"No se encontr\u00f3 {coverage_path}. Ejecuta el bloque que genera la matriz de cobertura.\")\n\n    cov = pd.read_csv(coverage_path)\n    # columnas esperadas: service_type, year, month, url, has_parquet, http_status, content_length, checked_at\n    cov['service_type'] = cov['service_type'].str.lower()\n    cov['year'] = cov['year'].astype(int)\n    cov['month'] = cov['month'].astype(int)\n    cov['has_parquet'] = cov['has_parquet'].astype(bool)\n\n    # 2) Consultar conteos por mes (s\u00f3lo para los que tienen parquet)\n    conn = _conn()\n    cs = conn.cursor()\n    try:\n        cs.execute(DDL_AUDIT.format(db=DB, schema=SCHEMA))\n\n        # armar DF audit\n        records = []\n        # Preparamos listas de meses a contar por servicio\n        to_check_yellow = cov[(cov['service_type']=='yellow') & (cov['has_parquet']==True)][['year','month']].drop_duplicates()\n        to_check_green  = cov[(cov['service_type']=='green')  & (cov['has_parquet']==True)][['year','month']].drop_duplicates()\n\n        # Hacemos una sola query por servicio (m\u00e1s eficiente)\n        def fetch_counts(table_name: str):\n            q = f\"\"\"\n            select year, month,\n                   count(*) as row_count,\n                   max(try_to_timestamp(ingest_ts)) as latest_ingest_ts\n            from {DB}.{SCHEMA}.{table_name}\n            group by 1,2\n            \"\"\"\n            cs.execute(q)\n            rows = cs.fetchall()\n            # dict {(year,month): (row_count, latest_ingest_ts)}\n            return {(r[0], r[1]): (int(r[2] or 0), r[3]) for r in rows}\n\n        yellow_counts = fetch_counts('yellow_trips')\n        green_counts  = fetch_counts('green_trips')\n\n        # 3) Determinar status fila por fila\n        for idx, r in cov.iterrows():\n            svc = r['service_type']; y = int(r['year']); m = int(r['month'])\n            if not r['has_parquet']:\n                records.append({\n                    'service_type': svc,\n                    'year': y, 'month': m,\n                    'row_count': 0,\n                    'latest_ingest_ts': None,\n                    'status': 'MISSING',\n                    'note': 'No Parquet available'\n                })\n            else:\n                if svc == 'yellow':\n                    rc, ts = yellow_counts.get((y,m), (0, None))\n                else:\n                    rc, ts = green_counts.get((y,m), (0, None))\n                status = 'OK' if rc > 0 else 'PENDING'\n                records.append({\n                    'service_type': svc,\n                    'year': y, 'month': m,\n                    'row_count': rc,\n                    'latest_ingest_ts': ts,\n                    'status': status,\n                    'note': None\n                })\n\n        audit_df = pd.DataFrame(records)\n        audit_df.sort_values(['service_type','year','month'], inplace=True)\n\n        # 4) Reemplazar contenido de LOAD_AUDIT\n        cs.execute(f\"truncate table {DB}.{SCHEMA}.load_audit\")\n        ok, nchunks, nrows, _ = write_pandas(\n            conn,\n            audit_df,\n            table_name='load_audit',\n            database=DB,\n            schema=SCHEMA,\n            quote_identifiers=False,\n            chunk_size=50_000,\n        )\n        print(f\"[audit] Actualizado ok={ok}, filas={nrows}, chunks={nchunks}\")\n\n    finally:\n        try: cs.close()\n        except Exception: pass\n        conn.close()\n", "file_path": "data_exporters/sync_coverage_to_audit_py.py", "language": "python", "type": "data_exporter", "uuid": "sync_coverage_to_audit_py"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "dbts/dbt_setup.yaml:dbt:yaml:dbt setup": {"content": "--select fct_trips", "file_path": "dbts/dbt_setup.yaml", "language": "yaml", "type": "dbt", "uuid": "dbt_setup"}, "transformers/build_coverage_matrix.py:transformer:python:build coverage matrix": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nfrom mage_ai.settings.repo import get_repo_path\nimport os\nimport time\nimport random\n\n# Constantes\nSERVICES_ORDER = ['green', 'yellow']   # procesar green primero, luego yellow\nYEARS = list(range(2015, 2026))\nMONTHS = list(range(1, 13))\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\ndef _check_parquet_with_retries(url: str, max_attempts=3, timeout=(4,15), base_sleep=0.5):\n    \"\"\"\n    HEAD con reintentos/backoff. Devuelve (has_parquet, status, content_length, notes).\n    Considera reintentar en caso de 403/5xx/transitorios.\n    \"\"\"\n    headers = {\n        # usar User-Agent \"razonable\" para evitar bloqueos b\u00e1sicos\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n                      '(KHTML, like Gecko) Chrome/116.0 Safari/537.36'\n    }\n\n    attempt = 0\n    last_exc = None\n    while attempt < max_attempts:\n        attempt += 1\n        try:\n            r = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)\n            status = r.status_code\n            content_length = None\n            if 'Content-Length' in r.headers:\n                try:\n                    content_length = int(r.headers.get('Content-Length') or 0)\n                except Exception:\n                    content_length = None\n\n            # Heur\u00edstica: si 200 y content_length>0 (o None) -> OK\n            if status == 200 and (content_length is None or content_length > 0):\n                return True, status, content_length, None\n            if status in (403, 404):\n                # Puede ser transitorio: reintentamos algunas veces para 403, pero despu\u00e9s lo marcamos como missing\n                notes = 'missing' if status == 404 else 'forbidden'\n                # reintentar si no es el \u00faltimo intento y es 403 (posible bloqueo temporal)\n                if status == 403 and attempt < max_attempts:\n                    sleep = base_sleep * (2 ** (attempt - 1)) + random.random() * 0.5\n                    time.sleep(sleep)\n                    continue\n                return False, status, content_length, notes\n\n            # Otros c\u00f3digos (5xx etc) -> reintentar\n            notes = f'unexpected_status_{status}'\n            if 500 <= status < 600 and attempt < max_attempts:\n                sleep = base_sleep * (2 ** (attempt - 1)) + random.random() * 0.5\n                time.sleep(sleep)\n                continue\n            return False, status, content_length, notes\n\n        except Exception as e:\n            last_exc = e\n            # reintentar en caso de excepciones transitorias\n            if attempt < max_attempts:\n                sleep = base_sleep * (2 ** (attempt - 1)) + random.random() * 0.5\n                time.sleep(sleep)\n                continue\n            return False, None, None, f'error:{type(e).__name__}'\n\n@transformer\ndef transform(*args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Coverage matrix: procesa green primero, luego yellow; reintentos para 403/transitorios;\n    guarda un \u00fanico CSV unificado en repo ('coverage_matrix.csv').\n    \"\"\"\n    rows = []\n    now_iso = datetime.utcnow().isoformat(timespec='seconds') + 'Z'\n\n    for service in SERVICES_ORDER:\n        for year in YEARS:\n            for month in MONTHS:\n                url = _build_url(service, year, month)\n                has_parquet, http_status, content_length, notes = _check_parquet_with_retries(url)\n                rows.append({\n                    'service_type': service,\n                    'year': year,\n                    'month': month,\n                    'url': url,\n                    'has_parquet': has_parquet,\n                    'http_status': http_status,\n                    'content_length': content_length,\n                    'checked_at': now_iso,\n                    'notes': notes,\n                })\n\n                # pausa corta para evitar bursts y throttling\n                time.sleep(0.08)  # ~80ms entre requests; ajusta si necesitas m\u00e1s lento\n\n    df = pd.DataFrame(rows)\n\n    # Guardar CSV en el repo (unificado)\n    try:\n        repo = get_repo_path()\n        out_path = os.path.join(repo, 'coverage_matrix.csv')\n        df.to_csv(out_path, index=False)\n        print(f\"[coverage] Escrib\u00ed {len(df)} filas en {out_path}\")\n    except Exception as e:\n        print(f\"[coverage][warning] No pude escribir CSV: {e}\")\n\n    return df\n", "file_path": "transformers/build_coverage_matrix.py", "language": "python", "type": "transformer", "uuid": "build_coverage_matrix"}, "transformers/creative_resonance.py:transformer:python:creative resonance": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\n\n@transformer\ndef transform(*args, **kwargs):\n    \"\"\"\n    Prueba conexi\u00f3n a Snowflake con Mage Secrets.\n    Retorna informaci\u00f3n b\u00e1sica de la sesi\u00f3n actual.\n    \"\"\"\n    conn = snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_RAW'),\n    )\n    cs = conn.cursor()\n    try:\n        cs.execute(\"\"\"\n            select current_user(), current_role(), current_warehouse(),\n                   current_database(), current_schema();\n        \"\"\")\n        row = cs.fetchone()\n        print(\"Conexi\u00f3n exitosa \u2705\")\n        print(\"User:\", row[0])\n        print(\"Role:\", row[1])\n        print(\"Warehouse:\", row[2])\n        print(\"Database:\", row[3])\n        print(\"Schema:\", row[4])\n        return {\n            'user': row[0],\n            'role': row[1],\n            'warehouse': row[2],\n            'database': row[3],\n            'schema': row[4],\n        }\n    finally:\n        cs.close()\n        conn.close()\n", "file_path": "transformers/creative_resonance.py", "language": "python", "type": "transformer", "uuid": "creative_resonance"}, "transformers/fetch_and_stage_parquet.py:transformer:python:fetch and stage parquet": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport requests\nfrom datetime import datetime\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    # Formato oficial: yellow_tripdata_YYYY-MM.parquet / green_tripdata_YYYY-MM.parquet\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\n@transformer\ndef transform(data: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Verifica si existe el archivo Parquet para cada (year, month, service_type).\n    NO descarga; usa peticiones HEAD para ahorrar tiempo/ancho de banda.\n\n    Input:\n        data: DataFrame del bloque generate_months con columnas:\n              ['year', 'month', 'service_type']\n\n    Output:\n        DataFrame con columnas:\n          ['year','month','service_type','url','has_parquet','http_status',\n           'content_length','checked_at','notes']\n    \"\"\"\n    rows_out = []\n    now_iso = datetime.utcnow().isoformat(timespec='seconds') + 'Z'\n\n    # Seguridad: asegurarnos de que vienen las columnas esperadas\n    required_cols = {'year', 'month', 'service_type'}\n    missing = required_cols - set(data.columns)\n    if missing:\n        raise ValueError(f\"Faltan columnas en la entrada del bloque: {missing}\")\n\n    # FILTRO a\u00f1os completos\n    #data = data.copy()\n    #Query de prueba para a\u00f1o\n    data = data.query(\"year == 2025\").copy()\n    #Query a\u00f1o y servicio\n    #data = data.query(\"year == 2019 and service_type == 'green' and month == 12\").copy()\n\n\n\n\n    for _, r in data.iterrows():\n        year = int(r['year'])\n        month = int(r['month'])\n        service = str(r['service_type']).lower().strip()\n\n        url = _build_url(service, year, month)\n        http_status = None\n        has_parquet = False\n        content_length = None\n        notes = None\n\n        try:\n            resp = requests.head(url, allow_redirects=True, timeout=10)\n            http_status = resp.status_code\n            # Algunos endpoints devuelven Content-Length:\n            if 'Content-Length' in resp.headers:\n                try:\n                    content_length = int(resp.headers['Content-Length'])\n                except Exception:\n                    content_length = None\n\n            if resp.status_code == 200:\n                has_parquet = True\n            elif resp.status_code in (403, 404):\n                has_parquet = False\n                notes = 'missing'\n            else:\n                has_parquet = False\n                notes = f'unexpected_status_{resp.status_code}'\n        except Exception as e:\n            http_status = None\n            has_parquet = False\n            notes = f'error:{type(e).__name__}'\n\n        rows_out.append({\n            'year': year,\n            'month': month,\n            'service_type': service,\n            'url': url,\n            'has_parquet': has_parquet,\n            'http_status': http_status,\n            'content_length': content_length,\n            'checked_at': now_iso,\n            'notes': notes,\n        })\n\n    return pd.DataFrame(rows_out)\n\n\n@test\ndef test_output(output: pd.DataFrame, *args) -> None:\n    \"\"\"\n    Validaciones b\u00e1sicas del resultado.\n    \"\"\"\n    assert output is not None, 'El output es None'\n    assert len(output) > 0, 'El output est\u00e1 vac\u00edo'\n\n    expected_cols = {\n        'year', 'month', 'service_type', 'url',\n        'has_parquet', 'http_status', 'content_length',\n        'checked_at', 'notes'\n    }\n    missing = expected_cols - set(output.columns)\n    assert not missing, f'Faltan columnas esperadas: {missing}'\n\n    # Tipos b\u00e1sicos\n    assert output['year'].dtype.kind in 'iu', 'year debe ser entero'\n    assert output['month'].dtype.kind in 'iu', 'month debe ser entero'\n    assert output['service_type'].dtype.kind in 'OSU', 'service_type debe ser texto'\n    assert output['url'].str.contains('http').all(), 'Hay URLs inv\u00e1lidas'\n", "file_path": "transformers/fetch_and_stage_parquet.py", "language": "python", "type": "transformer", "uuid": "fetch_and_stage_parquet"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/generate_months.py:transformer:python:generate months": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n\n@transformer\ndef transform(*args, **kwargs):\n    \"\"\"\n    Genera la matriz de (year, month, service_type) para 2015\u20132025.\n    Esto servir\u00e1 como entrada para los siguientes bloques de ingesta.\n    \"\"\"\n    years = range(2015, 2026)  # 2015\u20132025 inclusive\n    services = ['yellow', 'green']\n    rows = []\n\n    for y in years:\n        for m in range(1, 13):\n            for s in services:\n                rows.append({'year': y, 'month': m, 'service_type': s})\n\n    # Devolvemos un DataFrame para que Mage lo pueda manejar abajo\n    return pd.DataFrame(rows)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Verifica que el bloque produjo datos v\u00e1lidos.\n    \"\"\"\n    # Debe haber 11 a\u00f1os * 12 meses * 2 servicios = 264 filas\n    assert len(output) == 264, f'Esperaba 264 filas, encontr\u00e9 {len(output)}'\n\n    # Validar que contiene las columnas esperadas\n    for col in ['year', 'month', 'service_type']:\n        assert col in output.columns, f'Falta columna {col}'\n", "file_path": "transformers/generate_months.py", "language": "python", "type": "transformer", "uuid": "generate_months"}, "transformers/snowflake_connection.py:transformer:python:snowflake connection": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport os\nimport snowflake.connector as sf\n\n@transformer\ndef test_snowflake_connection(*args, **kwargs):\n    conn = sf.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_SILVER'),\n        insecure_mode=True,\n    )\n    try:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT CURRENT_ROLE(), CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA();\")\n            print('[SF CONNECTED]', cur.fetchone())\n    finally:\n        conn.close()\n    return \"OK\"\n\n", "file_path": "transformers/snowflake_connection.py", "language": "python", "type": "transformer", "uuid": "snowflake_connection"}, "transformers/sync_coverage_to_audit.py:transformer:python:sync coverage to audit": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/sync_coverage_to_audit.py", "language": "python", "type": "transformer", "uuid": "sync_coverage_to_audit"}, "transformers/update_coverage.py:transformer:python:update coverage": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport os\nimport pandas as pd\nfrom datetime import datetime\n\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nCOVERAGE_PATH = \"/home/src/docs/coverage_matrix.csv\"\n\ndef _conn():\n    return snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=get_secret_value('SNOWFLAKE_SCHEMA_RAW'),\n        client_session_keep_alive=False,\n    )\n\ndef _read_existing_csv():\n    if os.path.exists(COVERAGE_PATH):\n        try:\n            return pd.read_csv(COVERAGE_PATH)\n        except Exception:\n            pass\n    return pd.DataFrame(columns=[\n        'year','month','service_type','has_parquet','load_status',\n        'row_count','notes','updated_at'\n    ])\n\ndef _counts_from_snowflake():\n    db = get_secret_value('SNOWFLAKE_DATABASE')\n    sch = get_secret_value('SNOWFLAKE_SCHEMA_RAW')\n\n    conn = _conn()\n    try:\n        q = f\"\"\"\n        with y as (\n          select 'yellow' as service_type, year, month, count(*) as row_count\n          from {db}.{sch}.yellow_trips\n          group by 1,2,3\n        ),\n        g as (\n          select 'green' as service_type, year, month, count(*) as row_count\n          from {db}.{sch}.green_trips\n          group by 1,2,3\n        )\n        select * from y\n        union all\n        select * from g\n        \"\"\"\n        df = pd.read_sql(q, conn)\n        # asegurar tipos\n        if not df.empty:\n            df['year'] = df['year'].astype(int)\n            df['month'] = df['month'].astype(int)\n            df['service_type'] = df['service_type'].astype(str)\n        return df\n    except Exception as e:\n        print(\"Error consultando conteos en Snowflake:\", e)\n        return pd.DataFrame(columns=['service_type','year','month','row_count'])\n    finally:\n        conn.close()\n\n@transformer\ndef transform(availability_df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    availability_df: output del bloque 2 con columnas:\n      ['year','month','service_type','url','has_parquet','http_status','content_length','checked_at','notes']\n\n    Efecto:\n      - Escribe/actualiza docs/coverage_matrix.csv\n    Retorna:\n      - El DataFrame completo de cobertura actualizado (\u00fatil para inspecci\u00f3n en UI).\n    \"\"\"\n    if availability_df is None or availability_df.empty:\n        raise ValueError(\"No lleg\u00f3 availability_df desde el bloque anterior.\")\n\n    # Normaliza columnas importantes\n    avail = availability_df[['year','month','service_type','has_parquet','notes']].copy()\n    avail['year'] = avail['year'].astype(int)\n    avail['month'] = avail['month'].astype(int)\n    avail['service_type'] = avail['service_type'].astype(str)\n\n    # Conteos actuales en BRONZE\n    counts = _counts_from_snowflake()\n\n    # Merge availability + counts\n    cov = avail.merge(\n        counts,\n        on=['service_type','year','month'],\n        how='left'\n    )\n    cov['row_count'] = cov['row_count'].fillna(0).astype(int)\n\n    # Determinar load_status\n    def decide_status(row):\n        if pd.notnull(row.get('notes')) and str(row['notes']).startswith('error'):\n            return 'error'\n        if not row['has_parquet']:\n            return 'missing'\n        # has_parquet True:\n        return 'ok' if row['row_count'] > 0 else 'pending'\n\n    cov['load_status'] = cov.apply(decide_status, axis=1)\n    cov['updated_at'] = datetime.utcnow().isoformat(timespec='seconds') + 'Z'\n\n    # Cargar cobertura previa y hacer upsert por clave (y,m,service)\n    prev = _read_existing_csv()\n    key_cols = ['year','month','service_type']\n    if not prev.empty:\n        # drop duplicados en prev y cov, priorizando los nuevos\n        prev = prev.drop_duplicates(subset=key_cols, keep='last')\n        cov = cov.drop_duplicates(subset=key_cols, keep='last')\n        merged = pd.concat([prev, cov], ignore_index=True)\n        merged = merged.sort_values('updated_at').drop_duplicates(subset=key_cols, keep='last')\n    else:\n        merged = cov\n\n    # Guardar CSV\n    os.makedirs(os.path.dirname(COVERAGE_PATH), exist_ok=True)\n    merged = merged[['year','month','service_type','has_parquet','load_status','row_count','notes','updated_at']]\n    merged.to_csv(COVERAGE_PATH, index=False)\n\n    print(f\"Cobertura actualizada: {COVERAGE_PATH} ({len(merged)} filas)\")\n    return merged\n\n@test\ndef test_output(output: pd.DataFrame, *args) -> None:\n    assert output is not None and not output.empty, \"Output vac\u00edo.\"\n    for c in ['year','month','service_type','has_parquet','load_status','row_count','updated_at']:\n        assert c in output.columns, f\"Falta columna {c}\"\n", "file_path": "transformers/update_coverage.py", "language": "python", "type": "transformer", "uuid": "update_coverage"}, "pipelines/ingest_backfill_2015_2025/metadata.yaml:pipeline:yaml:ingest backfill 2015 2025/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - fetch_and_stage_parquet\n  - snowflake_connection\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: generate_months\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks: []\n  uuid: generate_months\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: snowflake_connection\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - generate_months\n  uuid: snowflake_connection\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - copy_into_bronze\n  - build_coverage_matrix\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: fetch_and_stage_parquet\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - generate_months\n  uuid: fetch_and_stage_parquet\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: build_coverage_matrix\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - fetch_and_stage_parquet\n  uuid: build_coverage_matrix\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - load_taxi_zones\n  - sync_coverage_to_audit_py\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: copy_into_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - fetch_and_stage_parquet\n  uuid: copy_into_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: sync_coverage_to_audit.py\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - copy_into_bronze\n  uuid: sync_coverage_to_audit_py\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_taxi_zones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - copy_into_bronze\n  uuid: load_taxi_zones\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt:\n      command: run\n    dbt_profile_target: gold\n    dbt_project_name: dbt/nyc_tlc\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/dbt_setup.yaml\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: dbt_setup\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt_setup\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/nyc_tlc\n    file_path: dbt/nyc_tlc/models/staging/stg_green.sql\n    file_source:\n      path: dbt/nyc_tlc/models/staging/stg_green.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n  downstream_blocks:\n  - dbt/nyc_tlc/models/core/silver_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/staging/stg_green\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/staging/stg_green\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/nyc_tlc\n    file_path: dbt/nyc_tlc/models/staging/stg_yellow.sql\n    file_source:\n      path: dbt/nyc_tlc/models/staging/stg_yellow.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n  downstream_blocks:\n  - dbt/nyc_tlc/models/core/silver_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/staging/stg_yellow\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/staging/stg_yellow\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/nyc_tlc\n    file_path: dbt/nyc_tlc/models/lookups/payment_type_lookup.sql\n    file_source:\n      path: dbt/nyc_tlc/models/lookups/payment_type_lookup.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n  downstream_blocks:\n  - dbt/nyc_tlc/models/core/silver_trips\n  - dbt/nyc_tlc/models/marts/dim_payment_type\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/lookups/payment_type_lookup\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/lookups/payment_type_lookup\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/nyc_tlc\n    file_path: dbt/nyc_tlc/models/lookups/ratecode_lookup.sql\n    file_source:\n      path: dbt/nyc_tlc/models/lookups/ratecode_lookup.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n  downstream_blocks:\n  - dbt/nyc_tlc/models/core/silver_trips\n  - dbt/nyc_tlc/models/marts/dim_ratecode\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/lookups/ratecode_lookup\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/lookups/ratecode_lookup\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/nyc_tlc\n    file_path: dbt/nyc_tlc/models/core/silver_trips.sql\n    file_source:\n      path: dbt/nyc_tlc/models/core/silver_trips.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/core/silver_trips\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/nyc_tlc/models/staging/stg_green\n  - dbt/nyc_tlc/models/lookups/payment_type_lookup\n  - dbt/nyc_tlc/models/staging/stg_yellow\n  - dbt/nyc_tlc/models/lookups/ratecode_lookup\n  uuid: dbt/nyc_tlc/models/core/silver_trips\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: gold\n    dbt_project_name: dbt/nyc_tlc\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/nyc_tlc/models/marts/dim_zone.sql\n    file_source:\n      path: dbt/nyc_tlc/models/marts/dim_zone.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/marts/dim_zone\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/marts/dim_zone\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: gold\n    dbt_project_name: dbt/nyc_tlc\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/nyc_tlc/models/marts/dim_ratecode.sql\n    file_source:\n      path: dbt/nyc_tlc/models/marts/dim_ratecode.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/marts/dim_ratecode\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/nyc_tlc/models/lookups/ratecode_lookup\n  uuid: dbt/nyc_tlc/models/marts/dim_ratecode\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: gold\n    dbt_project_name: dbt/nyc_tlc\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/nyc_tlc/models/marts/dim_payment_type.sql\n    file_source:\n      path: dbt/nyc_tlc/models/marts/dim_payment_type.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/marts/dim_payment_type\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/nyc_tlc/models/lookups/payment_type_lookup\n  uuid: dbt/nyc_tlc/models/marts/dim_payment_type\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: gold\n    dbt_project_name: dbt/nyc_tlc\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/nyc_tlc/models/marts/fct_trips.sql\n    file_source:\n      path: dbt/nyc_tlc/models/marts/fct_trips.sql\n      project_path: dbt/nyc_tlc\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/nyc_tlc/models/marts/fct_trips\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/nyc_tlc/models/marts/fct_trips\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-21 18:13:49.573282+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ingest_backfill_2015_2025\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ingest_backfill_2015_2025\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/ingest_backfill_2015_2025/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ingest_backfill_2015_2025/metadata"}, "pipelines/ingest_backfill_2015_2025/__init__.py:pipeline:python:ingest backfill 2015 2025/  init  ": {"content": "", "file_path": "pipelines/ingest_backfill_2015_2025/__init__.py", "language": "python", "type": "pipeline", "uuid": "ingest_backfill_2015_2025/__init__"}, "/home/src/default_repo/data_exporters/sync_coverage_to_audit_py.py:data_exporter:python:home/src/default repo/data exporters/sync coverage to audit py": {"content": "# --- guard del template de Mage ---\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom mage_ai.settings.repo import get_repo_path\n\nimport os\nimport pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom datetime import datetime\n\n# ============== Conexi\u00f3n ==============\ndef _conn(schema_override=None):\n    # SIN fallback: siempre RAW (o lo que pases expl\u00edcitamente en schema_override)\n    schema = schema_override or get_secret_value('SNOWFLAKE_SCHEMA_RAW')\n    return snowflake.connector.connect(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema=schema,\n        client_session_keep_alive=False,\n        ocsp_fail_open=True,\n        insecure_mode=True,\n    )\n\n# ============== DDLs m\u00ednimas + ensure columns ==============\nDDL_AUDIT_MIN = \"\"\"\ncreate table if not exists {db}.{schema}.load_audit (\n  service_type string,\n  year int,\n  month int\n);\n\"\"\"\n\nDDL_COVERAGE_MIN = \"\"\"\ncreate table if not exists {db}.{schema}.coverage_matrix (\n  service_type string,\n  year int,\n  month int\n);\n\"\"\"\n\ndef _ensure_audit_columns(conn, db, schema, table='load_audit'):\n    fq = f\"{db}.{schema}.{table}\"\n    cur = conn.cursor()\n    try:\n        cur.execute(f\"alter table if exists {fq} add column if not exists row_count number\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists latest_ingest_ts timestamp_ntz\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists status string\")  # OK | MISSING\n        cur.execute(f\"alter table if exists {fq} add column if not exists note string\")\n    finally:\n        cur.close()\n\ndef _ensure_coverage_columns(conn, db, schema, table='coverage_matrix'):\n    fq = f\"{db}.{schema}.{table}\"\n    cur = conn.cursor()\n    try:\n        cur.execute(f\"alter table if exists {fq} add column if not exists url string\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists has_parquet boolean\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists http_status int\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists content_length number(38,0)\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists checked_at timestamp_ntz\")\n        cur.execute(f\"alter table if exists {fq} add column if not exists notes string\")\n    finally:\n        cur.close()\n\n# ============== Helpers ==============\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\ndef _grid_to_df(services, years_from, years_to):\n    rows = []\n    for svc in services:\n        for y in range(int(years_from), int(years_to) + 1):\n            for m in range(1, 13):\n                rows.append((svc, y, m))\n    return pd.DataFrame(rows, columns=['service_type','year','month'])\n\ndef _values_rows_int(int_iterable):\n    # genera: (2015),(2016),...  -> m\u00faltiples filas (correcto para Snowflake VALUES)\n    return \", \".join(f\"({int(v)})\" for v in int_iterable)\n\n# ============== Exportador principal ==============\n@data_exporter\ndef export_data(*args, **kwargs) -> None:\n    \"\"\"\n    Construye/actualiza LOAD_AUDIT y COVERAGE_MATRIX directamente desde RAW.\n    kwargs:\n      - years_from: int (default 2015)\n      - years_to:   int (default 2025)\n      - services:   list[str] (default ['green','yellow'])\n      - schema:     str (default: secreto SNOWFLAKE_SCHEMA_RAW)  -> SIN fallback\n      - truncate:   bool (default True)  -> TRUNCATE + INSERT\n      - write_csv:  bool (default True)  -> guarda coverage_matrix.csv en el repo\n    \"\"\"\n    DB = get_secret_value('SNOWFLAKE_DATABASE')\n    SCHEMA = kwargs.get('schema') or get_secret_value('SNOWFLAKE_SCHEMA_RAW')  # SIN fallback\n\n    years_from = int(kwargs.get('years_from', 2015))\n    years_to   = int(kwargs.get('years_to', 2025))\n    services   = [s.lower() for s in kwargs.get('services', ['green','yellow']) if s.lower() in ('green','yellow')]\n    if not services:\n        services = ['green','yellow']\n    truncate   = bool(kwargs.get('truncate', True))\n    write_csv  = bool(kwargs.get('write_csv', True))\n\n    # 1) Armar malla completa\n    base = _grid_to_df(services, years_from, years_to)\n\n    # 2) SQL de conteos desde RAW (VALUES como filas)\n    services_vals = \", \".join([f\"('{s}')\" for s in services])\n    years_rows  = _values_rows_int(range(years_from, years_to + 1))  # -> (2015),(2016),...\n    months_rows = _values_rows_int(range(1, 13))                     # -> (1),(2),...\n\n    sql_counts = f\"\"\"\nwith services(service_type) as (\n  select column1 from values {services_vals}\n),\nyears(year) as (\n  select column1 from values {years_rows}\n),\nmonths(month) as (\n  select column1 from values {months_rows}\n),\nbase as (\n  select s.service_type, y.year, m.month\n  from services s\n  cross join years y\n  cross join months m\n),\ncounts as (\n  select 'green' as service_type, year, month,\n         count(*) as row_count,\n         max(try_to_timestamp(ingest_ts)) as latest_ingest_ts\n  from {DB}.{SCHEMA}.green_trips\n  where year between {years_from} and {years_to}\n  group by 1,2,3\n  union all\n  select 'yellow' as service_type, year, month,\n         count(*) as row_count,\n         max(try_to_timestamp(ingest_ts)) as latest_ingest_ts\n  from {DB}.{SCHEMA}.yellow_trips\n  where year between {years_from} and {years_to}\n  group by 1,2,3\n)\nselect\n  b.service_type,\n  b.year,\n  b.month,\n  coalesce(c.row_count, 0) as row_count,\n  c.latest_ingest_ts as latest_ingest_ts\nfrom base b\nleft join counts c\n  on b.service_type = c.service_type\n and b.year = c.year\n and b.month = c.month\norder by b.service_type, b.year, b.month\n\"\"\"\n\n    conn = _conn(schema_override=SCHEMA)\n    cur = conn.cursor()\n    try:\n        # 3) Ejecutar conteos\n        cur.execute(sql_counts)\n        df_counts: pd.DataFrame = cur.fetch_pandas_all()\n        df_counts.columns = [c.lower() for c in df_counts.columns]\n        df_counts['row_count'] = df_counts['row_count'].fillna(0).astype(int)\n\n        # 4) Construir LOAD_AUDIT\n        audit_df = df_counts.copy()\n        audit_df['status'] = audit_df['row_count'].apply(lambda x: 'OK' if x > 0 else 'MISSING')\n        audit_df['note'] = pd.NA\n        audit_df = audit_df[['service_type','year','month','row_count','latest_ingest_ts','status','note']]\n\n        # 5) Construir COVERAGE_MATRIX (basado en audit_df)\n        now_ntz = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n        cov_df = audit_df[['service_type','year','month','row_count']].copy()\n        cov_df['url'] = cov_df.apply(lambda r: _build_url(r['service_type'], int(r['year']), int(r['month'])), axis=1)\n        cov_df['has_parquet'] = cov_df['row_count'] > 0\n        cov_df['http_status'] = cov_df['has_parquet'].apply(lambda v: 200 if bool(v) else None)\n        cov_df['content_length'] = pd.NA\n        cov_df['checked_at'] = pd.to_datetime(now_ntz)  # NTZ\n        cov_df['notes'] = 'from_raw'\n        cov_df = cov_df[['service_type','year','month','url','has_parquet','http_status','content_length','checked_at','notes']]\n\n        # 6) Asegurar tablas y columnas\n        cur.execute(DDL_AUDIT_MIN.format(db=DB, schema=SCHEMA))\n        _ensure_audit_columns(conn, DB, SCHEMA)\n\n        cur.execute(DDL_COVERAGE_MIN.format(db=DB, schema=SCHEMA))\n        _ensure_coverage_columns(conn, DB, SCHEMA)\n\n        # 7) Escribir en Snowflake (TRUNCATE + INSERT por defecto)\n        fq_audit = f\"{DB}.{SCHEMA}.load_audit\"\n        fq_cov   = f\"{DB}.{SCHEMA}.coverage_matrix\"\n\n        if truncate:\n            cur.execute(f\"truncate table {fq_audit}\")\n            cur.execute(f\"truncate table {fq_cov}\")\n        else:\n            # delete selectivo para la malla solicitada\n            keys = \", \".join([f\"('{r.service_type}',{int(r.year)},{int(r.month)})\" for r in base.itertuples(index=False)])\n            if keys:\n                cur.execute(f\"delete from {fq_audit} where (service_type,year,month) in ({keys})\")\n                cur.execute(f\"delete from {fq_cov}   where (service_type,year,month) in ({keys})\")\n\n        ok1, c1, n1, _ = write_pandas(conn, audit_df, table_name='load_audit', database=DB, schema=SCHEMA, quote_identifiers=False, chunk_size=100_000)\n        ok2, c2, n2, _ = write_pandas(conn, cov_df,   table_name='coverage_matrix', database=DB, schema=SCHEMA, quote_identifiers=False, chunk_size=100_000)\n        print(f\"[load_audit] ok={ok1}, rows={n1}, chunks={c1}\")\n        print(f\"[coverage_matrix] ok={ok2}, rows={n2}, chunks={c2}\")\n\n        # 8) (Opcional) Guardar coverage_matrix.csv en el repo\n        if write_csv:\n            out_path = os.path.join(get_repo_path(), 'coverage_matrix.csv')\n            tmp = out_path + \".tmp\"\n            cov_df.to_csv(tmp, index=False)\n            os.replace(tmp, out_path)\n            print(f\"[coverage_matrix] CSV escrito en {out_path}\")\n\n    finally:\n        try: cur.close()\n        except Exception: pass\n        conn.close()\n", "file_path": "/home/src/default_repo/data_exporters/sync_coverage_to_audit_py.py", "language": "python", "type": "data_exporter", "uuid": "sync_coverage_to_audit_py"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}